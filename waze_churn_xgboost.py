# -*- coding: utf-8 -*-
"""Waze_Churn_XGBoost.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zyNOtpqTR-JgZGEYn1q3Iz0zQ-3HzbdF
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report,ConfusionMatrixDisplay

import zipfile

# Unzip the dataset
with zipfile.ZipFile('/content/waze_dataset.csv.zip', 'r') as zip_ref:
    zip_ref.extractall('dataset_directory')

df=pd.read_csv('/content/dataset_directory/waze_dataset.csv')

df

df.shape

df.drop('ID', axis=1, inplace=True)

df['label'].value_counts(normalize=True)*100

label_counts = df['label'].value_counts(dropna=False)
print(label_counts)

df.describe()

# Create `km_per_driving_day` column
df['km_per_driving_day'] = df['driven_km_drives'] / df['driving_days']

# Convert infinite values to zero
df.loc[df['km_per_driving_day'] == np.inf, 'km_per_driving_day'] = 0

# Descriptive Statistic
df['km_per_driving_day'].describe()

# Create `professional_driver` column
df['professional_driver'] = np.where((df['drives']>=60) & (df['driving_days']>= 15), 1, 0)

df[['drives', 'driving_days', 'professional_driver']].head()

# Check count of professionals and non-professionals
print(df['professional_driver'].value_counts())

# Check in-class churn rate
df.groupby(['professional_driver'])['label'].value_counts(normalize=True) * 100

# Check info()
df.info()

# Drop rows with missing data in `label` column
df = df.dropna(subset=['label'], axis=0)

print()
# Data dimension
print('Data Dimension after removing missing value:', df.shape)
print()

# Check if there any missing data
print(df.isnull().sum())

# Impute Outliers
for column in ['sessions', 'drives', 'total_sessions', 'total_navigations_fav1',
               'total_navigations_fav2','driven_km_drives', 'duration_minutes_drives', 'km_per_driving_day']:
    threshold = df[column].quantile(0.95)
    df.loc[df[column]>threshold, column] = threshold

df.dropna()

print(df.isnull().sum().sum())

df.describe()

# Create binary `label2` column
df['label2'] = np.where(df['label']=='churned', 1, 0)
df[['label', 'label2']].tail()

# Create new `device2` variable
df['device2'] = np.where(df['device']=='Android', 0, 1)
df[['device', 'device2']].tail()

# Generate a correlation matrix
df.corr(method='pearson', numeric_only=True)

# Plot correlation heatmap
import seaborn as sns
plt.figure(figsize=(14, 10))
sns.heatmap(df.corr(method='pearson', numeric_only=True),
            vmin=-1, vmax=1, annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap', fontsize=14, weight='bold');

# prompt: drop drives and device
df = df.drop(['drives'], axis=1)
df = df.drop(['driving_days'], axis=1)
df = df.drop(['driven_km_drives'], axis=1)

import sklearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
import pandas as pd # Import pandas

X = df.drop('label2', axis=1)
y = df['label2']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a OneHotEncoder object
encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore') # sparse=False for numpy array

# Fit the encoder on the training data and transform both training and testing data
X_train_encoded = encoder.fit_transform(X_train[['device']]) # Apply to 'device' column
X_test_encoded = encoder.transform(X_test[['device']])

# Get feature names from the encoder
feature_names = encoder.get_feature_names_out(['device'])

# Create DataFrames from the encoded data
X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=feature_names, index=X_train.index)
X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=feature_names, index=X_test.index)

# Drop the original 'device' column and concatenate the encoded columns
X_train = X_train.drop('device', axis=1)
X_train = pd.concat([X_train, X_train_encoded_df], axis=1)
X_test = X_test.drop('device', axis=1)
X_test = pd.concat([X_test, X_test_encoded_df], axis=1)

# Drop the 'label' column before scaling
X_train = X_train.drop('label', axis=1) # Drop 'label' column from X_train
X_test = X_test.drop('label', axis=1)   # Drop 'label' column from X_test


# Now apply MinMaxScaler
StandardScaler = MinMaxScaler() # It's actually MinMaxScaler
X_train = StandardScaler.fit_transform(X_train)
X_test = StandardScaler.transform(X_test)

# Apply SMOTE
!pip install imbalanced-learn
from imblearn.over_sampling import SMOTE # Import SMOTE class from imblearn.over_sampling
smote = SMOTE(sampling_strategy='auto')
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Select only numerical features for scaling
numerical_features = df.select_dtypes(include=['number']).columns
df_numerical = df[numerical_features]

# Apply MinMaxScaler to the numerical features
scaler_minmax = MinMaxScaler()
scaled_data_minmax = scaler_minmax.fit_transform(df_numerical)

# Create a new DataFrame with scaled numerical features
df_scaled = pd.DataFrame(scaled_data_minmax, columns=numerical_features, index=df.index)

# Concatenate scaled numerical features with original non-numerical features
df_final = pd.concat([df.drop(columns=numerical_features), df_scaled], axis=1)

from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.ensemble import RandomForestClassifier

print("\nTraining and Evaluating XGBoost Model...")
xgb_model = XGBClassifier(random_state=42, class_weight='balanced',learning_rate=0.1, max_depth=9, n_estimators=150)
xgb_model.fit(X_train,y_train)
y_pred_xgb = xgb_model.predict(X_test)

# Evaluate XGBoost
print("XGBoost Accuracy:", accuracy_score(y_test, y_pred_xgb))
print("XGBoost Classification Report:")
print(classification_report(y_test, y_pred_xgb))

# prompt: visualize the imbalance of the data

import matplotlib.pyplot as plt
import seaborn as sns

# Count the occurrences of each label
label_counts = df['label'].value_counts()

# Create a bar plot to visualize the label distribution
plt.figure(figsize=(8, 6))
sns.countplot(x='label', data=df)
plt.title('Label Distribution')
plt.xlabel('Label')
plt.ylabel('Count')
plt.show()

# Print the label counts for a more precise understanding
print(label_counts)

# Create binary `label2` column
df['label2'] = np.where(df['label']=='churned', 1, 0)
df[['label', 'label2']].tail()

import numpy as np
from imblearn.combine import SMOTEENN
from sklearn.utils.class_weight import compute_class_weight
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder  # Import LabelEncoder

# Apply SMOTE to the training data
smote_enn = SMOTEENN(random_state=42)
X_train_smote, y_train_smote = smote_enn.fit_resample(X_train, y_train)

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Fit LabelEncoder on the original y_train (before SMOTE)
label_encoder.fit(y_train) # Fit on original y_train with string labels

# Transform the target variables using the fitted LabelEncoder
y_train_smote = label_encoder.transform(y_train_smote)
y_train = label_encoder.transform(y_train) # Ensure y_train is also transformed for class_weight calculation
y_test_encoded = label_encoder.transform(y_test) # Transform y_test using the fitted encoder


# Calculate class weights
class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)
class_weights = dict(zip(np.unique(y_train), class_weights)) # Convert to dictionary

model = XGBClassifier(random_state=42,class_weight='balanced')

# Train the model with SMOTE-resampled data and class weights
model.fit(X_train_smote, y_train_smote , sample_weight=[class_weights[label] for label in y_train_smote])

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model using the encoded y_test
accuracy = accuracy_score(y_test_encoded, y_pred)  # Use y_test_encoded here
print("Accuracy with SMOTE:", accuracy)
print(classification_report(y_test_encoded, y_pred)) # Use y_test_encoded here as well

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Display the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels=['retained', 'churned'])
disp.plot();